taxero {
  word.embeddings: src/test/resources/embeddings/vectors.txt
  lemmatize: true
}

odinson {
  data.dir: ./target
  annotator {
    processor: fast_nlp

    dynet {
      mem.mb: 512
      autobatch: true
    }
  }

  extractor {
    rulesHome: src/test/resources/rules
    state: mock
    lemmatize: true
  }

  compiler {
    aggressiveNormalization = true
  }

  index {
    dir: ${odinson.data.dir}/test_index
    maxNumberOfTokensPerSentence: 100
    synchronizeOrderWithDocumentId: false
    invalidCharacterReplacement: "\ufffd"
  }

  fields {
    rawTokenField: raw
    wordTokenField: word
    lemmaTokenField: lemma
    posTagTokenField: tag
    chunkTokenField: chunk
    entityTokenField: entity
    dependenciesField: dependencies
    incomingTokenField: incoming
    outgoingTokenField: outgoing
    documentIdField: docId
    sentenceIdField: sentId
    sentenceLengthField: numWords
    displayField: raw
    storedFields: [${odinson.fields.displayField}]
    normalizedTokenField: norm
    addToNormalizedField: [
      ${odinson.fields.rawTokenField},
      ${odinson.fields.wordTokenField},
    ]
  }

}
